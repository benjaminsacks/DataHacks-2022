{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# nlp libraries\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$SPY wouldn't be surprised to see a green close</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shell's $70 Billion BG Deal Meets Shareholder ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SSH COMMUNICATIONS SECURITY CORP STOCK EXCHANG...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment\n",
       "0  According to the Finnish-Russian Chamber of Co...   neutral\n",
       "1  The Swedish buyout firm has sold its remaining...   neutral\n",
       "2    $SPY wouldn't be surprised to see a green close  positive\n",
       "3  Shell's $70 Billion BG Deal Meets Shareholder ...  negative\n",
       "4  SSH COMMUNICATIONS SECURITY CORP STOCK EXCHANG...  negative"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('advanced_trainset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of texts is also sometimes called \"corpus\". Let's print the first ten messages and number them using **enumerate**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    According to the Finnish-Russian Chamber of Co...\n",
       "1    The Swedish buyout firm has sold its remaining...\n",
       "2      $SPY wouldn't be surprised to see a green close\n",
       "3    Shell's $70 Billion BG Deal Meets Shareholder ...\n",
       "4    SSH COMMUNICATIONS SECURITY CORP STOCK EXCHANG...\n",
       "5    The Stockmann department store will have a tot...\n",
       "6    Circulation revenue has increased by 5 % in Fi...\n",
       "7    $SAP Q1 disappoints as #software licenses down...\n",
       "8    The subdivision made sales revenues last year ...\n",
       "9             Viking Line has canceled some services .\n",
       "Name: Sentence, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentence[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 According to the Finnish-Russian Chamber of Commerce , all the major construction companies of Finland are operating in Russia .\n",
      "\n",
      "\n",
      "1 The Swedish buyout firm has sold its remaining 22.4 percent stake , almost eighteen months after taking the company public in Finland .\n",
      "\n",
      "\n",
      "2 $SPY wouldn't be surprised to see a green close\n",
      "\n",
      "\n",
      "3 Shell's $70 Billion BG Deal Meets Shareholder Skepticism\n",
      "\n",
      "\n",
      "4 SSH COMMUNICATIONS SECURITY CORP STOCK EXCHANGE RELEASE OCTOBER 14 , 2008 AT 2:45 PM The Company updates its full year outlook and estimates its results to remain at loss for the full year .\n",
      "\n",
      "\n",
      "5 The Stockmann department store will have a total floor space of over 8,000 square metres and Stockmann 's investment in the project will have a price tag of about EUR 12 million .\n",
      "\n",
      "\n",
      "6 Circulation revenue has increased by 5 % in Finland and 4 % in Sweden in 2008 .\n",
      "\n",
      "\n",
      "7 $SAP Q1 disappoints as #software licenses down. Real problem? #Cloud growth trails $MSFT $ORCL $GOOG $CRM $ADBE https://t.co/jNDphllzq5\n",
      "\n",
      "\n",
      "8 The subdivision made sales revenues last year of EUR 480.7 million EUR 414.9 million in 2008 , and operating profits of EUR 44.5 million EUR 7.4 million .\n",
      "\n",
      "\n",
      "9 Viking Line has canceled some services .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message_no, message in enumerate(df.Sentence[:10]):\n",
    "    print(message_no, message)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use **read_csv** and make note of the **sep** argument, we can also specify the desired column names by passing in a list of *names*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's check out some of the stats with some plots and the built-in methods in pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4382</td>\n",
       "      <td>4382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4081</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>In the first half of 2008 , the Bank 's operat...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence Sentiment\n",
       "count                                                4382      4382\n",
       "unique                                               4081         3\n",
       "top     In the first half of 2008 , the Bank 's operat...   neutral\n",
       "freq                                                    2      2363"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use **groupby** to use describe by label, this way we can begin to think about the features that separate ham and spam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>636</td>\n",
       "      <td>636</td>\n",
       "      <td>Shell's $70 Billion BG Deal Meets Shareholder ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>2363</td>\n",
       "      <td>2360</td>\n",
       "      <td>The company serves customers in various indust...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "      <td>$SPY wouldn't be surprised to see a green close</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sentence                                                            \\\n",
       "             count unique                                                top   \n",
       "Sentiment                                                                      \n",
       "negative       636    636  Shell's $70 Billion BG Deal Meets Shareholder ...   \n",
       "neutral       2363   2360  The company serves customers in various indust...   \n",
       "positive      1383   1383    $SPY wouldn't be surprised to see a green close   \n",
       "\n",
       "                \n",
       "          freq  \n",
       "Sentiment       \n",
       "negative     1  \n",
       "neutral      2  \n",
       "positive     1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Sentiment').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how long the text messages are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$SPY wouldn't be surprised to see a green close</td>\n",
       "      <td>positive</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shell's $70 Billion BG Deal Meets Shareholder ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SSH COMMUNICATIONS SECURITY CORP STOCK EXCHANG...</td>\n",
       "      <td>negative</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment  length\n",
       "0  According to the Finnish-Russian Chamber of Co...   neutral     128\n",
       "1  The Swedish buyout firm has sold its remaining...   neutral     135\n",
       "2    $SPY wouldn't be surprised to see a green close  positive      47\n",
       "3  Shell's $70 Billion BG Deal Meets Shareholder ...  negative      56\n",
       "4  SSH COMMUNICATIONS SECURITY CORP STOCK EXCHANG...  negative     190"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df['Sentence'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['length'].plot(bins=50, kind='hist') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortest message only contains up to quite a bit more than 300 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4382.000000\n",
       "mean      117.269968\n",
       "std        56.811408\n",
       "min         9.000000\n",
       "25%        72.000000\n",
       "50%       107.000000\n",
       "75%       151.000000\n",
       "max       315.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supported Nokia phones include : N96 , N95-8GB , N95 , N93-N931 , N92 , N85 , N82 , N81 , N80 , N79 , N78 , N77 , N76 , N75 , N73 , N72 , N71 , E90 , E71 , E70 , E66 , E65 , E62 , E61-E61i , E60 , E51 , E50 , Touch Xpress 5800 , 6220 Classic , 6210 Navigator , 6120 Classic , 6110 Navigator , 5700 , 5500 , 5320XM .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what's in the longest message\n",
    "df[df['length'] == 315]['Sentence'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how those sentences with their sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'negative'}>,\n",
       "        <AxesSubplot:title={'center':'neutral'}>],\n",
       "       [<AxesSubplot:title={'center':'positive'}>, <AxesSubplot:>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(column='length', by='Sentiment', bins=50,figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything follows a common trend: peaks around 50-100\n",
    "\n",
    "neutral is unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuation\n",
    "\n",
    "Taking ad of Python's built-in **string**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Messs Hello Dollar sign $70 with other punctuations'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "mess = 'Messs! Hello: Dollar sign $70 with other punctuations.'\n",
    "\n",
    "# Check characters to see if they are in punctuation\n",
    "nopunc = [char for char in mess if char not in string.punctuation or '$' in char]\n",
    "\n",
    "# Join the characters again to form the string.\n",
    "nopunc = ''.join(nopunc)\n",
    "nopunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to remove stopwords. We can impot a list of english stopwords from NLTK (check the documentation for more languages and info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[0:11] # Show some stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['messs', 'hello', 'dollar', 'sign', '$70', 'punctuations']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any stopwords & make it lowercase\n",
    "clean_mess = [word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('english') or '$' in word.lower()]\n",
    "clean_mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put both of these together in a function to apply it to our DataFrame later on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ben function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(s):\n",
    "    # To lowercase\n",
    "    s = s.lower()\n",
    "\n",
    "    # Remove apostrophes\n",
    "    s = re.sub(' \\'s', '', s)\n",
    "\n",
    "    # Fix % and $ whitespace\n",
    "    s = re.sub('(?<=\\d) %', '%', s)\n",
    "    s = re.sub('\\$ (?=\\d)', '$', s)\n",
    "\n",
    "    # Remove links\n",
    "    s = re.sub('http\\S+', ' ', s)\n",
    "\n",
    "    # Remove .'s not surrounded by numbers\n",
    "    s = re.sub('(?<!\\d)\\.|\\,(?!\\d)', ' ', s)\n",
    "\n",
    "    # Remove punctuation\n",
    "    s = re.sub('-|\\(|\\)', ' ', s)\n",
    "    s = re.sub('\\'|\\,|\\`', '', s)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    s = re.sub(' +', ' ', s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add your function here and just replace all the parameter in Countvectorizer in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "#     return a list\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "words = defaultdict(int)\n",
    "for i in cleaned[\"Sentence_cleaned\"]:\n",
    "    for i in i.split():\n",
    "        words[i] += 1\n",
    "\n",
    "words\n",
    "word_index = {}\n",
    "counter = 0\n",
    "for i in words:\n",
    "    if words[i] > 1:\n",
    "        word_index[i] = counter\n",
    "        counter += 1\n",
    "word_index[\" \"] = counter\n",
    "\n",
    "word_count = []\n",
    "for sentence in cleaned[\"Sentence_cleaned\"]:\n",
    "    result = [0] * 5225\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in word_index:\n",
    "            result[word_index[word]] += 1\n",
    "    word_count.append(result)\n",
    "new_df = df.copy()\n",
    "new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process_with_dollar(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation or '$' in char]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords & chekc if dollar sign in it\n",
    "    return [word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('english') or '$' in word.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is just the term used to describe the process of converting the normal text strings in to a list of tokens (words that we actually want).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [According, FinnishRussian, Chamber, Commerce,...\n",
       "1    [Swedish, buyout, firm, sold, remaining, 224, ...\n",
       "2         [SPY, wouldnt, surprised, see, green, close]\n",
       "3    [Shells, 70, Billion, BG, Deal, Meets, Shareho...\n",
       "4    [SSH, COMMUNICATIONS, SECURITY, CORP, STOCK, E...\n",
       "Name: Sentence, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure its working\n",
    "df['Sentence'].head(5).apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$SPY wouldn't be surprised to see a green close</td>\n",
       "      <td>positive</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shell's $70 Billion BG Deal Meets Shareholder ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SSH COMMUNICATIONS SECURITY CORP STOCK EXCHANG...</td>\n",
       "      <td>negative</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment  length\n",
       "0  According to the Finnish-Russian Chamber of Co...   neutral     128\n",
       "1  The Swedish buyout firm has sold its remaining...   neutral     135\n",
       "2    $SPY wouldn't be surprised to see a green close  positive      47\n",
       "3  Shell's $70 Billion BG Deal Meets Shareholder ...  negative      56\n",
       "4  SSH COMMUNICATIONS SECURITY CORP STOCK EXCHANG...  negative     190"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show original dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing Normalization\n",
    "\n",
    "There are a lot of ways to continue normalizing this text. Such as [Stemming](https://en.wikipedia.org/wiki/Stemming) or distinguishing by [part of speech](http://www.nltk.org/book/ch05.html).\n",
    "\n",
    "NLTK has lots of built-in tools and great documentation on a lot of these methods. Sometimes they don't work well for text-messages due to the way a lot of people tend to use abbreviations or shorthand, For example:\n",
    "    \n",
    "    'Nah dawg, IDK! Wut time u headin to da club?'\n",
    "    \n",
    "versus\n",
    "\n",
    "    'No dog, I don't know! What time are you heading to the club?'\n",
    "    \n",
    "Some text normalization methods will have trouble with this type of shorthand and so I'll leave you to explore those more advanced methods through the [NLTK book online](http://www.nltk.org/book/).\n",
    "\n",
    "For now we will just focus on using what we have to convert our list of words to an actual vector that SciKit-Learn can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we have the messages as lists of tokens (also known as [lemmas](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)) and now we need to convert each of those messages into a vector the SciKit Learn's algorithm models can work with.\n",
    "\n",
    "Now we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n",
    "\n",
    "We'll do that in three steps using the bag-of-words model:\n",
    "\n",
    "1. Count how many times does a word occur in each message (Known as term frequency)\n",
    "\n",
    "2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "\n",
    "3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "\n",
    "Let's begin the first step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector will have as many dimensions as there are unique words in the SMS corpus.  We will first use SciKit Learn's **CountVectorizer**. This model will convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "We can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message. \n",
    "\n",
    "For example:\n",
    "\n",
    "<table border = “1“>\n",
    "<tr>\n",
    "<th></th> <th>Message 1</th> <th>Message 2</th> <th>...</th> <th>Message N</th> \n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word 1 Count</b></td><td>0</td><td>1</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word 2 Count</b></td><td>0</td><td>0</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>...</b></td> <td>1</td><td>2</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word N Count</b></td> <td>0</td><td>1</td><td>...</td><td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Since there are so many messages, we can expect a lot of zero counts for the presence of that word in that document. Because of this, SciKit Learn will output a [Sparse Matrix](https://en.wikipedia.org/wiki/Sparse_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of arguments and parameters that can be passed to the CountVectorizer. In this case we will just specify the **analyzer** to be our own previously defined function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12073\n"
     ]
    }
   ],
   "source": [
    "# Take a while\n",
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(df['Sentence'])\n",
    "\n",
    "# Show total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take one text message and get its bag-of-words counts as a vector, putting to use our new `bow_transformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shell's $70 Billion BG Deal Meets Shareholder Skepticism\n"
     ]
    }
   ],
   "source": [
    "message4 = df['Sentence'][3]\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see its vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1194)\t1\n",
      "  (0, 1746)\t1\n",
      "  (0, 1868)\t1\n",
      "  (0, 2409)\t1\n",
      "  (0, 4157)\t1\n",
      "  (0, 5407)\t1\n",
      "  (0, 5414)\t1\n",
      "  (0, 5468)\t1\n",
      "(1, 12073)\n"
     ]
    }
   ],
   "source": [
    "bow4 = bow_transformer.transform([message4])\n",
    "print(bow4)\n",
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "004\n",
      "043\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names()[10])\n",
    "print(bow_transformer.get_feature_names()[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use **.transform** on our Bag-of-Words (bow) transformed object and transform the entire DataFrame of messages. Let's go ahead and check out how the bag-of-words counts for the entire SMS corpus is a large, sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(df['Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (4382, 12073)\n",
      "Amount of Non-Zero occurences:  51709\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', messages_bow.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0\n"
     ]
    }
   ],
   "source": [
    "sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))\n",
    "print('sparsity: {}'.format(round(sparsity)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the counting, the term weighting and normalization can be done with [TF-IDF](http://en.wikipedia.org/wiki/Tf%E2%80%93idf), using scikit-learn's `TfidfTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5468)\t0.39629165715027104\n",
      "  (0, 5414)\t0.3778061386922472\n",
      "  (0, 5407)\t0.3778061386922472\n",
      "  (0, 4157)\t0.39629165715027104\n",
      "  (0, 2409)\t0.31460373363129207\n",
      "  (0, 1868)\t0.33308925208931595\n",
      "  (0, 1746)\t0.32291593716252964\n",
      "  (0, 1194)\t0.29365324199362997\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "tfidf4 = tfidf_transformer.transform(bow4)\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go ahead and check what is the IDF (inverse document frequency) of the word `\"u\"`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.692341519858864\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['u']])\n",
    "# print(tfidf_transformer.idf_[bow_transformer.vocabulary_['']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the entire bag-of-words corpus into TF-IDF corpus at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4382, 12073)\n"
     ]
    }
   ],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With messages represented as vectors, we can finally train our classifier. Now we can actually use almost any sort of classification algorithms. For a [variety of reasons](http://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn-note07-2up.pdf), the Naive Bayes classifier algorithm is a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using scikit-learn here, choosing the [Naive Bayes](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifier to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(messages_tfidf, df['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try classifying our single random message and checking how we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: positive\n",
      "expected: negative\n"
     ]
    }
   ],
   "source": [
    "print('predicted:', spam_detect_model.predict(tfidf4)[0])\n",
    "print('expected:', df.Sentiment[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! We've developed a model that can attempt to predict spam vs ham classification!\n",
    "\n",
    "## Part 6: Model Evaluation\n",
    "Now we want to determine how well our model will do overall on the entire dataset. Let's begin by getting all the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'neutral' 'positive' ... 'neutral' 'neutral' 'positive']\n"
     ]
    }
   ],
   "source": [
    "all_predictions = spam_detect_model.predict(messages_tfidf)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use SciKit Learn's built-in classification report, which returns [precision, recall,](https://en.wikipedia.org/wiki/Precision_and_recall) [f1-score](https://en.wikipedia.org/wiki/F1_score), and a column for support (meaning how many cases supported that classification). Check out the links for more detailed info on each of these metrics and the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png' width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.18      0.30       636\n",
      "     neutral       0.72      1.00      0.84      2363\n",
      "    positive       0.91      0.66      0.76      1383\n",
      "\n",
      "    accuracy                           0.77      4382\n",
      "   macro avg       0.88      0.61      0.63      4382\n",
      "weighted avg       0.82      0.77      0.74      4382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(df['Sentiment'], all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77110908261068"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(df['Sentiment'], all_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3505 877 4382\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "msg_train, msg_test, label_train, label_test = \\\n",
    "train_test_split(df['Sentence'], df['Sentiment'], test_size=0.2)\n",
    "\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test size is 20% of the entire dataset, and the training is the rest. Note the default split would have been 30/70.\n",
    "\n",
    "## Creating a Data Pipeline\n",
    "\n",
    "Let's run our model again and then predict off the test set. We will use SciKit Learn's [pipeline](http://scikit-learn.org/stable/modules/pipeline.html) capabilities to store a pipeline of workflow. This will allow us to set up all the transformations that we will do to the data for future use. Let's see an example of how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_naive_bayes = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "#     ('decision_tree',  DecisionTreeClassifier(random_state=0)),\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_tree = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('decision_tree',  DecisionTreeClassifier(random_state=0)),\n",
    "#     ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_adaboost = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('ada_boost',  AdaBoostClassifier()),\n",
    "#     ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can directly pass message text data and the pipeline will do our pre-processing for us! We can treat it as a model/estimator API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x7fb65d0770d0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('ada_boost', AdaBoostClassifier())])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_naive_bayes.fit(msg_train,label_train)\n",
    "pipeline_tree.fit(msg_train,label_train)\n",
    "pipeline_adaboost.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_naive_bayes = pipeline_naive_bayes.predict(msg_test)\n",
    "predictions_tree = pipeline_tree.predict(msg_test)\n",
    "predictions_adaboost = pipeline_adaboost.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = df.copy()\n",
    "# new_df['pred_nb'] = predictions_naive_bayes\n",
    "# new_df['pred_tree'] = predictions_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.26      0.27      0.26       131\n",
      "     neutral       0.69      0.65      0.67       485\n",
      "    positive       0.58      0.64      0.61       261\n",
      "\n",
      "    accuracy                           0.59       877\n",
      "   macro avg       0.51      0.52      0.51       877\n",
      "weighted avg       0.59      0.59      0.59       877\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5895096921322691"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tree\n",
    "print(classification_report(predictions_tree,label_test))\n",
    "accuracy_score(label_test, predictions_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.01      0.67      0.03         3\n",
      "     neutral       0.98      0.60      0.74       752\n",
      "    positive       0.33      0.77      0.46       122\n",
      "\n",
      "    accuracy                           0.62       877\n",
      "   macro avg       0.44      0.68      0.41       877\n",
      "weighted avg       0.89      0.62      0.70       877\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.621436716077537"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "print(classification_report(predictions_naive_bayes,label_test))\n",
    "accuracy_score(label_test, predictions_naive_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.26      0.46      0.33        76\n",
      "     neutral       0.91      0.61      0.73       686\n",
      "    positive       0.33      0.83      0.47       115\n",
      "\n",
      "    accuracy                           0.62       877\n",
      "   macro avg       0.50      0.63      0.51       877\n",
      "weighted avg       0.78      0.62      0.66       877\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6237172177879133"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ada_boost\n",
    "print(classification_report(predictions_adaboost,label_test))\n",
    "accuracy_score(label_test, predictions_adaboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ben functions classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_naive_bayes = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=clean_sentence)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "#     ('decision_tree',  DecisionTreeClassifier(random_state=0)),\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "pipeline_tree = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('decision_tree',  DecisionTreeClassifier(random_state=0)),\n",
    "#     ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "pipeline_adaboost = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('ada_boost',  AdaBoostClassifier()),\n",
    "#     ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x7fb65d0770d0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('ada_boost', AdaBoostClassifier())])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_naive_bayes.fit(msg_train,label_train)\n",
    "pipeline_tree.fit(msg_train,label_train)\n",
    "pipeline_adaboost.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_naive_bayes = pipeline_naive_bayes.predict(msg_test)\n",
    "predictions_tree = pipeline_tree.predict(msg_test)\n",
    "predictions_adaboost = pipeline_adaboost.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.26      0.27      0.26       131\n",
      "     neutral       0.69      0.65      0.67       485\n",
      "    positive       0.58      0.64      0.61       261\n",
      "\n",
      "    accuracy                           0.59       877\n",
      "   macro avg       0.51      0.52      0.51       877\n",
      "weighted avg       0.59      0.59      0.59       877\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5895096921322691"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tree\n",
    "print(classification_report(predictions_tree,label_test))\n",
    "accuracy_score(label_test, predictions_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "     neutral       0.99      0.55      0.70       828\n",
      "    positive       0.10      0.62      0.18        48\n",
      "\n",
      "    accuracy                           0.55       877\n",
      "   macro avg       0.37      0.39      0.29       877\n",
      "weighted avg       0.94      0.55      0.67       877\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5496009122006842"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "print(classification_report(predictions_naive_bayes,label_test))\n",
    "accuracy_score(label_test, predictions_naive_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.26      0.46      0.33        76\n",
      "     neutral       0.91      0.61      0.73       686\n",
      "    positive       0.33      0.83      0.47       115\n",
      "\n",
      "    accuracy                           0.62       877\n",
      "   macro avg       0.50      0.63      0.51       877\n",
      "weighted avg       0.78      0.62      0.66       877\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6237172177879133"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ada_boost\n",
    "print(classification_report(predictions_adaboost,label_test))\n",
    "accuracy_score(label_test, predictions_adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
